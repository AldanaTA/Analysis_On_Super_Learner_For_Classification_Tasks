{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2995ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import tuning_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7e146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordVector:\n",
    "    def __init__(self,vector,index = 0,key = '<pad>'):\n",
    "        self.index = index\n",
    "        self.key = key\n",
    "        self.vector = vector\n",
    "def get_embeddings(wv_objs):\n",
    "    temp = []\n",
    "    for objs in wv_objs.values():\n",
    "        temp.append(objs.vector)\n",
    "    return np.array(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76672966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(targets,preds):\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average='micro',zero_division=0.0)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e54ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_Train_Lables(label):\n",
    "    temp = np.zeros(5)\n",
    "    temp[label-1] = 1\n",
    "    return temp\n",
    "def encode_Test_Lables(label):\n",
    "    return int(label-1)\n",
    "\n",
    "def splitsentences(sentences):\n",
    "    splits = []\n",
    "    for x in range(len(sentences)):\n",
    "        splits.append(sentences[x].split())\n",
    "    return splits\n",
    "def encodeSentence(sentence,embeds,max):\n",
    "    encoding = np.zeros(max)\n",
    "    for i in range(len(encoding)):\n",
    "        index = 0\n",
    "        if i < len(sentence):\n",
    "            try:\n",
    "                index = embeds[sentence[i]].index\n",
    "            except:\n",
    "                index = 0\n",
    "        encoding[i] = index\n",
    "    return encoding.astype(int)    \n",
    "def encodeSentences(data,embeds,max):\n",
    "    splitted_sentences = splitsentences(data)\n",
    "    encoded_Sentences = []\n",
    "    for sentence in splitted_sentences:\n",
    "        encoded_Sentences.append(encodeSentence(sentence,embeds,max))\n",
    "    return np.array(encoded_Sentences)\n",
    "def shrinkEmbeds(corpus,embeds):\n",
    "    words_index = dict()\n",
    "    words_index['<pad>'] = wordVector(embeds[0],0,'<pad>')\n",
    "    i = 1\n",
    "    for word in corpus:\n",
    "        if word not in words_index:\n",
    "            try:\n",
    "                words_index[word] = wordVector(embeds[word],i,word)\n",
    "                i += 1\n",
    "            except: \n",
    "                pass \n",
    "    return words_index\n",
    "def prep_data(filename,samp_size,embeds,min_df,max_df):\n",
    "  #reading Data\n",
    "    current_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    path = parent_dir +\"/model_data/\" + filename\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    #Sampling data\n",
    "    if samp_size > 1:\n",
    "        samp_size = 1\n",
    "    if samp_size <=0.1:\n",
    "      samp_size = .1\n",
    "\n",
    "    if int(len(df) * samp_size) <= 5000:\n",
    "        df = df.sample(n=int(len(df) * samp_size))\n",
    "    else:\n",
    "        df = df.sample(n=5000)\n",
    "\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    #preping features\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df,max_df=max_df)\n",
    "    corpus = vectorizer.fit(df[\"Review_text\"]).get_feature_names_out()\n",
    "    embeds = shrinkEmbeds(corpus,embeds)\n",
    "    mean = int(df[\"Review_text\"].str.split().str.len().mean())\n",
    "\n",
    "    X = encodeSentences(df[\"Review_text\"],embeds,mean)\n",
    "    #del vectorizer,text_vector\n",
    "    #gc.collect()\n",
    "\n",
    "    #preping labels\n",
    "    Y = df[\"Rating\"]\n",
    "    \n",
    "    \n",
    "    return X,Y,embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b03f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self,embeddings='',vocabSize='',device='cpu',hidden_size = 10,dropout = .0,num_layers = 1,):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device= device\n",
    "        self.embeds = torch.nn.Embedding(vocabSize,300,_freeze=False,device=device)\n",
    "        self.embeds.weight = torch.nn.Parameter(torch.from_numpy(embeddings))\n",
    "        self.rnn = torch.nn.RNN(300, hidden_size,num_layers,dropout= dropout,nonlinearity=\"relu\", batch_first=True,device=device)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_size, 5)\n",
    "        self.softMax = torch.nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.embeds(x).to(torch.float32)\n",
    "        input = input.to(self.device)\n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size, device= self.device)\n",
    "        out, _ = self.rnn(input,h0)\n",
    "      \n",
    "        out = out[:, -1, :]\n",
    "       \n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(filename,sample_size,min_df,max_df,batch_size,epochs,lr,momentum,weight_decay,hidden_size,dropout,num_layers):\n",
    "    current_dir = os.getcwd() + '//embeds//'\n",
    "    embeddings = KeyedVectors.load_word2vec_format(current_dir+'GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
    "    filename = \"data_set_1.csv\"\n",
    "    X_data,Y_data,embeddings = prep_data(filename,sample_size,embeddings,min_df,max_df)\n",
    "    embeddings = get_embeddings(embeddings)\n",
    "    vocab_size = len(embeddings)\n",
    "\n",
    "    #setting paramaters\n",
    "    net = NeuralNetClassifier(\n",
    "    module = RNNClassifier,\n",
    "    criterion= torch.nn.CrossEntropyLoss,\n",
    "    optimizer = torch.optim.SGD,\n",
    "    module__embeddings=embeddings,\n",
    "    module__vocabSize=vocab_size,\n",
    "    module__hidden_size = 10,\n",
    "    module__dropout = .0,\n",
    "    module__num_layers = 1,\n",
    "    optimizer__weight_decay = .01,\n",
    "    optimizer__lr = .001,\n",
    "    optimizer__momentum = .003,\n",
    "    batch_size = 8,\n",
    "    max_epochs=10,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "\n",
    "    )\n",
    "\n",
    "    net.set_params(train_split=False, verbose=0)\n",
    "    parameters = {\n",
    "        'module__vocabSize': [vocab_size],\n",
    "        'module__embeddings': [embeddings],\n",
    "        'module__hidden_size': hidden_size,\n",
    "        'module__dropout' :  dropout,\n",
    "        'module__num_layers': num_layers,\n",
    "        'optimizer__weight_decay': weight_decay,\n",
    "        'optimizer__lr': lr,\n",
    "        'optimizer__momentum' : momentum,\n",
    "        'batch_size': batch_size,\n",
    "        'max_epochs': epochs,\n",
    "        \n",
    "    }\n",
    "\n",
    "    tuning_script.grid_search(net,parameters,X_data,Y_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1656cdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m dropout = [\u001b[32m.1\u001b[39m,\u001b[32m.2\u001b[39m,\u001b[32m.3\u001b[39m]\n\u001b[32m     18\u001b[39m num_layers = [\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhiddensize\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mrun_experiments\u001b[39m\u001b[34m(filename, device, sample_size, min_df, max_df, batch_size, epochs, lr, momentum, weight_decay, hidden_size, dropout, num_layers)\u001b[39m\n\u001b[32m     44\u001b[39m X_data = torch.tensor(X_data, device = device)\n\u001b[32m     45\u001b[39m Y_data = torch.tensor(Y_data, device = device)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mtuning_script\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\MachineLearning-Final-Project-Files\\models\\tuning_script.py:13\u001b[39m, in \u001b[36mgrid_search\u001b[39m\u001b[34m(model, parameters, X, Y)\u001b[39m\n\u001b[32m      6\u001b[39m     scorers = {\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: make_scorer(accuracy_score),\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m: make_scorer(precision_score,average=\u001b[33m\"\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m: make_scorer(recall_score,average=\u001b[33m\"\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m: make_scorer(f1_score,average=\u001b[33m\"\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m }\n\u001b[32m     12\u001b[39m     grid_obj = GridSearchCV(model, parameters, scoring=scorers, cv=\u001b[32m5\u001b[39m,refit=\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mgrid_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest Hyper paramaters: \u001b[39m\u001b[33m\"\u001b[39m,grid_obj.best_params_)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest Scores: \u001b[39m\u001b[38;5;132;01m{0:.2%}\u001b[39;00m\u001b[33m\"\u001b[39m.format(grid_obj.best_score_))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:933\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    929\u001b[39m params = _check_method_params(X, params=params)\n\u001b[32m    931\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._get_routed_params_for_fit(params)\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m cv_orig = \u001b[43mcheck_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m n_splits = cv_orig.get_n_splits(X, y, **routed_params.splitter.split)\n\u001b[32m    936\u001b[39m base_estimator = clone(\u001b[38;5;28mself\u001b[39m.estimator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2716\u001b[39m, in \u001b[36mcheck_cv\u001b[39m\u001b[34m(cv, y, classifier)\u001b[39m\n\u001b[32m   2711\u001b[39m cv = \u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m cv\n\u001b[32m   2712\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cv, numbers.Integral):\n\u001b[32m   2713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2714\u001b[39m         classifier\n\u001b[32m   2715\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2716\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   2717\u001b[39m     ):\n\u001b[32m   2718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m StratifiedKFold(cv)\n\u001b[32m   2719\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:333\u001b[39m, in \u001b[36mtype_of_target\u001b[39m\u001b[34m(y, input_name, raise_unknown)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33my cannot be class \u001b[39m\u001b[33m'\u001b[39m\u001b[33mSparseSeries\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mSparseArray\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmultilabel-indicator\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:172\u001b[39m, in \u001b[36mis_multilabel\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m    170\u001b[39m warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     y = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e).startswith(\u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1058\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    837\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tonya\\OneDrive\\Desktop\\MachineLearning Final Project Files\\.venv\\Lib\\site-packages\\torch\\_tensor.py:1225\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "\n",
    "filename = \"data_set_1.csv\"\n",
    "\n",
    "sample_size = 5000\n",
    "\n",
    "min_df = .001\n",
    "max_df = .97\n",
    "batch_size = [4,8,12,16]\n",
    "epochs = [5,10,15,]\n",
    "lr = [.01,.05,.1,.5,2]\n",
    "momentum = [.001,.01,.1]\n",
    "weight_decay = [.0001,.001,.01]\n",
    "hiddensize = [5,10,15]\n",
    "dropout = [.1,.2,.3]\n",
    "num_layers = [1,2,3]\n",
    "\n",
    "run_experiments(filename,sample_size,min_df,max_df,batch_size,epochs,lr,momentum,weight_decay,hiddensize,dropout,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157fb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper paramaters that can be changed\n",
    "batch_size = 50\n",
    "n_layers = 2\n",
    "epochs = 15\n",
    "sample_size = .25\n",
    "dropout = .05\n",
    "decay = .001\n",
    "chosen_lr = .00001\n",
    "chosen_momentum = .003\n",
    "hidden_size = 20\n",
    "min_df = .01\n",
    "max_df = .85\n",
    "device = \"cpu\"\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "X_data,Y_data,embeddings = prep_data(filename,sample_size,embeddings,min_df,max_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee9537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3aa702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation method\n",
    "def Evaluate(model,X_data,Y_data,epochs):\n",
    "    f_loss = []\n",
    "    acc = []\n",
    "    precision = []\n",
    "    f1 = []\n",
    "    recall = []\n",
    "    \n",
    "    kfold = KFold(shuffle=True)\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(X_data,Y_data)):\n",
    "        print(f\"FOLD {fold+1}\")\n",
    "\n",
    "        train_x = X_data[train_ids]\n",
    "        train_y = Y_data.iloc[train_ids].astype(int).apply(encode_Train_Lables).to_numpy()\n",
    "\n",
    "        val_x = X_data[test_ids]\n",
    "        val_y = Y_data.iloc[test_ids].apply(encode_Test_Lables).to_numpy()\n",
    "        \n",
    "        train_y = np.stack(train_y)\n",
    "      \n",
    "        train_x = train_x.astype(np.int32)\n",
    "        \n",
    "        val_x = val_x.astype(np.int32)\n",
    "        '''\n",
    "        val_y = np.stack(val_y)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "\n",
    "        train_dataset = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True,num_workers=4)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.from_numpy(val_x),torch.from_numpy(val_y))\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,pin_memory=True,num_workers=4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=chosen_lr,weight_decay=decay,momentum=chosen_momentum)\n",
    "\n",
    "        #Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for text,lables in train_dataloader:\n",
    "                text = text.to(device)\n",
    "                lables = lables.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                yhat = model(text)\n",
    "                loss = criterion(yhat,lables)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        #Results\n",
    "        with torch.no_grad():\n",
    "            totalLoss = 0\n",
    "            preds = []\n",
    "            targets = []\n",
    "            for text,lables in val_dataloader:\n",
    "       \n",
    "                text = text.to(device)\n",
    "                lables = lables.to(device)\n",
    "        \n",
    "                yhat = model(text)\n",
    "\n",
    "                loss = criterion(yhat,lables)\n",
    "\n",
    "                yhat = yhat.cpu()\n",
    "                lables = lables.cpu()\n",
    "                \n",
    "                preds.extend(torch.argmax(yhat,dim=1).numpy())\n",
    "                targets.extend(lables.numpy())\n",
    "                totalLoss+=loss.item()\n",
    "            results = getMetrics(targets,preds)\n",
    "            acc.append(results['accuracy'])\n",
    "            precision.append(results['precision'])\n",
    "            f1.append(results['f1'])\n",
    "            recall.append(results['recall'])\n",
    "            f_loss.append(totalLoss/len(val_dataloader))\n",
    "            print(\"Avg Total Loss: {0:.2f}\".format(totalLoss/len(val_dataloader)))\n",
    "            print(classification_report(targets,preds,zero_division=0.0))\n",
    "    acc = np.array(acc)\n",
    "    precision = np.array(precision)\n",
    "    f1 = np.array(f1)\n",
    "    recall = np.array(recall)\n",
    "    f_loss = np.array(f_loss)\n",
    "\n",
    "    print(\"Model Loss: {0:.4f} Accuracy {1:.2%} Precision: {2:.4f} Recall: {3:.4f} F1 Score: {4:.4f}\".format(np.mean(f_loss),np.mean(acc),np.mean(precision),np.mean(recall),np.mean(f1),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(embeddings,vocab_size,embed_size,hidden_size,dropout,n_layers,device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "Evaluate(model,X_data,Y_data,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e774f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_x,test_x,train_y,test_y = train_test_split(X_data,Y_data,test_size=.20)\n",
    "train_y = np.stack(train_y.astype(int).apply(encode_Train_Lables))\n",
    "train_x = train_x.astype(np.int32)\n",
    "\n",
    "val_y = test_y.apply(encode_Test_Lables).to_numpy()\n",
    "val_x = test_x.astype(np.int32)\n",
    "\n",
    "model = RNNClassifier(embeddings,vocab_size,embed_size,hidden_size,dropout,n_layers,device)\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True,num_workers=4)\n",
    "\n",
    "model.to(device)\n",
    "val_dataset = TensorDataset(torch.from_numpy(val_x),torch.from_numpy(val_y))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,pin_memory=True,num_workers=4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=chosen_lr,weight_decay=decay,momentum=chosen_momentum)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637faeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def getMetrics(targets,preds):\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average='macro',zero_division=0.0)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "#Training Loop\n",
    "for epoch in range(epochs):\n",
    "    if epoch > 0:\n",
    "        totalLoss = 0 \n",
    "        totalcorrect = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            targets = []\n",
    "            for text,lables in val_dataloader:\n",
    "                text = text.to(device)\n",
    "                lables = lables.to(device)\n",
    "                 \n",
    "                yhat = model(text)\n",
    "                \n",
    "                val_loss = criterion(yhat,lables)\n",
    "                \n",
    "                yhat = yhat.cpu()\n",
    "                lables = lables.cpu()\n",
    "\n",
    "                preds.extend(torch.argmax(yhat,dim=1).numpy())\n",
    "                targets.extend(lables.numpy())\n",
    "\n",
    "                totalLoss += val_loss.item() * len(val_dataloader)\n",
    "                \n",
    "            results = getMetrics(targets,preds)\n",
    "            del preds,targets\n",
    "            gc.collect()    \n",
    "            print(\"Epoch:  {0}   Loss: {1: .4f} Val Loss:   {2: .4f}  Val Accuracy: {3: .2%} Precision: {4: .2f} Recall: {5: .2f} f1 Score: {6: .2f}\".format(epoch,loss.item(),totalLoss/len(val_dataset),results['accuracy'],results['precision'],results['recall'],results['f1']))\n",
    "    model.train()    \n",
    "    for text,lables in train_dataloader:\n",
    "        text = text.to(device)\n",
    "        lables = lables.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = model(text)\n",
    "        loss = criterion(yhat,lables)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d61568",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "#Results\n",
    "\n",
    "with torch.no_grad():\n",
    "    totalLoss = 0\n",
    "    \n",
    "    preds = []\n",
    "    targets = []\n",
    "    for text,lables in val_dataloader:\n",
    "       \n",
    "        text = text.to(device)\n",
    "        lables = lables.to(device)\n",
    "        \n",
    "        yhat = model(text)\n",
    "\n",
    "        preds.extend(torch.argmax(yhat,dim=1).numpy())\n",
    "        targets.extend(lables.numpy())\n",
    "\n",
    "        loss = criterion(yhat,lables)\n",
    "        totalLoss+=loss.item()*len(val_dataloader)\n",
    "    \n",
    "    print(\"Total Loss: {0:.2f}\".format(totalLoss))\n",
    "    print(classification_report(targets,preds,zero_division=0.0))\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
