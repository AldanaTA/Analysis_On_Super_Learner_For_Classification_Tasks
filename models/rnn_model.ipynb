{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2995ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7e146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordVector:\n",
    "    def __init__(self,vector,index = 0,key = '<pad>'):\n",
    "        self.index = index\n",
    "        self.key = key\n",
    "        self.vector = vector\n",
    "def get_embeddings(wv_objs):\n",
    "    temp = []\n",
    "    for objs in wv_objs.values():\n",
    "        temp.append(objs.vector)\n",
    "    return np.array(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e54ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_Lables(label):\n",
    "    temp = np.zeros(5)\n",
    "    temp[label-1] = 1\n",
    "    return temp\n",
    "def splitsentences(sentences):\n",
    "    splits = []\n",
    "    for x in range(len(sentences)):\n",
    "        splits.append(sentences[x].split())\n",
    "    return splits\n",
    "def encodeSentence(sentence,embeds,max):\n",
    "    encoding = np.zeros(max)\n",
    "    for i in range(len(encoding)):\n",
    "        index = 0\n",
    "        if i < len(sentence):\n",
    "            try:\n",
    "                index = embeds[sentence[i]].index\n",
    "            except:\n",
    "                index = 0\n",
    "        encoding[i] = index\n",
    "    return encoding.astype(int)    \n",
    "def encodeSentences(data,embeds,max):\n",
    "    splitted_sentences = splitsentences(data)\n",
    "    encoded_Sentences = []\n",
    "    for sentence in splitted_sentences:\n",
    "        encoded_Sentences.append(encodeSentence(sentence,embeds,max))\n",
    "    return np.array(encoded_Sentences)\n",
    "def shrinkEmbeds(corpus,embeds):\n",
    "    words_index = dict()\n",
    "    words_index['<pad>'] = wordVector(embeds[0],0,'<pad>')\n",
    "    i = 1\n",
    "    for word in corpus:\n",
    "        if word not in words_index:\n",
    "            try:\n",
    "                words_index[word] = wordVector(embeds[word],i,word)\n",
    "                i += 1\n",
    "            except: \n",
    "                pass \n",
    "    return words_index\n",
    "def prep_data(filename,samp_size,embeds):\n",
    "  #reading Data\n",
    "    current_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    path = parent_dir +\"/model_data/\" + filename\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    #Sampling data\n",
    "    if samp_size > 1:\n",
    "        samp_size = 1\n",
    "    if samp_size <=0.1:\n",
    "      samp_size = .1\n",
    "    df = df.sample(n=int(len(df) * samp_size))\n",
    "    df.reset_index(inplace=True)\n",
    "    #preping features\n",
    "    vectorizer = TfidfVectorizer(min_df=1,max_df=.95)\n",
    "    corpus = vectorizer.fit(df[\"Review_text\"]).get_feature_names_out()\n",
    "    embeds = shrinkEmbeds(corpus,embeds)\n",
    "    mean = int(df[\"Review_text\"].str.split().str.len().mean())\n",
    "\n",
    "    X = encodeSentences(df[\"Review_text\"],embeds,mean)\n",
    "    #del vectorizer,text_vector\n",
    "    #gc.collect()\n",
    "\n",
    "    #preping labels\n",
    "    Y = df[\"Rating\"].astype(int).apply(encode_Lables)\n",
    "    \n",
    "    \n",
    "    return X,Y,embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b03f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self,embeddings,vocabSize,embedsize, hidden_size,dropout,num_layers,device):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device= device\n",
    "        self.embeds = torch.nn.Embedding(vocabSize,embedsize,_freeze=False,device=device)\n",
    "        self.embeds.weight = torch.nn.Parameter(torch.from_numpy(embeddings))\n",
    "        self.rnn = torch.nn.RNN(embedsize, hidden_size,num_layers,dropout= dropout,nonlinearity=\"relu\", batch_first=True)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_size, 5)\n",
    "        self.softMax = torch.nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.embeds(x).to(torch.float32)\n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size, device= self.device)\n",
    "        out, _ = self.rnn(input,h0)\n",
    "      \n",
    "        out = out[:, -1, :]\n",
    "       \n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1656cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() + '//embeds//'\n",
    "embeddings = KeyedVectors.load_word2vec_format(current_dir+'GoogleNews-vectors-negative300.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157fb656",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\taldan00\\\\Documents\\\\MLFP_DataAnalysis_and_DataPrep\\\\Code/model_data/data_set_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m     device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m filename = \u001b[33m\"\u001b[39m\u001b[33mdata_set_1.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m X_data,Y_data,embeddings = \u001b[43mprep_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mprep_data\u001b[39m\u001b[34m(filename, samp_size, embeds)\u001b[39m\n\u001b[32m     42\u001b[39m parent_dir = os.path.dirname(current_dir)\n\u001b[32m     43\u001b[39m path = parent_dir +\u001b[33m\"\u001b[39m\u001b[33m/model_data/\u001b[39m\u001b[33m\"\u001b[39m + filename\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m#Sampling data\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m samp_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taldan00\\Documents\\MLFP_DataAnalysis_and_DataPrep\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taldan00\\Documents\\MLFP_DataAnalysis_and_DataPrep\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taldan00\\Documents\\MLFP_DataAnalysis_and_DataPrep\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taldan00\\Documents\\MLFP_DataAnalysis_and_DataPrep\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taldan00\\Documents\\MLFP_DataAnalysis_and_DataPrep\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\taldan00\\\\Documents\\\\MLFP_DataAnalysis_and_DataPrep\\\\Code/model_data/data_set_1.csv'"
     ]
    }
   ],
   "source": [
    "#hyper paramaters that can be changed\n",
    "batch_size = 32\n",
    "n_layers = 2\n",
    "epochs = 10\n",
    "sample_size = .10\n",
    "dropout = .20\n",
    "decay = .001\n",
    "chosen_lr = .005\n",
    "chosen_momentum = .01\n",
    "hidden_size = 20\n",
    "device = \"cpu\"\n",
    "torch.set_default_dtype(torch.float32)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    device = 'cuda'\n",
    "\n",
    "\n",
    "filename = \"data_set_1.csv\"\n",
    "X_data,Y_data,embeddings = prep_data(filename,sample_size,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = len(embeddings)\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3535f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(X_data,Y_data,test_size=.20,random_state=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e774f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.stack(train_y.to_numpy())\n",
    "train_x = train_x.astype(np.int32)\n",
    "\n",
    "val_y = np.stack(test_y.to_numpy())\n",
    "val_x = test_x.astype(np.int32)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True,num_workers=4)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,pin_memory=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637faeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(embeddings,vocab_size,embed_size, hidden_size,dropout,n_layers,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=chosen_lr,weight_decay=decay,momentum=chosen_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumCorrect(predictions,labels):\n",
    "    sum = 0\n",
    "    for x in range(len(predictions)):\n",
    "        prediction = predictions[x]\n",
    "        yhat = torch.argmax(prediction)\n",
    "        correct = labels[x]\n",
    "        y = torch.argmax(correct)\n",
    "        if  yhat == y:\n",
    "            sum += 1\n",
    "    return sum\n",
    "model.to(device)\n",
    "printNum = int((len(train_dataset)/batch_size)*.2)\n",
    "\n",
    "model.train()\n",
    "#Training Loop\n",
    "for epoch in range(epochs):\n",
    "    BatchCounter = 0\n",
    "    for text,lables in train_dataloader:\n",
    "        BatchCounter += 1\n",
    "\n",
    "        text = text.to(device)\n",
    "        lables = lables.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yhat = model(text)\n",
    "        loss = criterion(yhat,lables)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if BatchCounter % printNum == 0:\n",
    "            totalLoss = 0 \n",
    "            totalcorrect = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for text,lables in val_dataloader:\n",
    "                    text = text.to(device)\n",
    "                 \n",
    "                    lables = lables.to(device)\n",
    "                 \n",
    "                    yhat = model(text)\n",
    "                    val_loss = criterion(yhat,lables)\n",
    "                    totalcorrect += sumCorrect(yhat,lables)\n",
    "                    totalLoss += val_loss.item() * len(val_dataloader)\n",
    "            model.train()\n",
    "            accuracy = totalcorrect/len(val_dataset)\n",
    "            print(\"Epoch:  {0} Batch:   {1}   Loss: {2: .4f} Val Loss:   {3: .4f}  Val Accuracy: {4: .2%}\".format(epoch + 1,BatchCounter,loss.item(),totalLoss/len(val_dataset),accuracy))\n",
    "\n",
    "#Results\n",
    "\n",
    "with torch.no_grad():\n",
    "    totalLoss = 0\n",
    "    totalcorrect = 0\n",
    "    for text,lables in val_dataloader:\n",
    "       \n",
    "        text = text.to(device)\n",
    "        lables = lables.to(device)\n",
    "        \n",
    "        yhat = model(text)\n",
    "        loss = criterion(yhat,lables)\n",
    "        totalcorrect += sumCorrect(yhat,lables)\n",
    "        totalLoss+=loss.item()*len(val_dataloader)\n",
    "    accuracy = totalcorrect/len(val_dataset)     \n",
    "    print(\"The total Loss is: {:.3}\".format(totalLoss/len(val_dataset)))\n",
    "    print(\"The Accuracy of the model is: {:.2%}\".format(accuracy))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
