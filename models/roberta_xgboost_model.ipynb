{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer,RobertaForSequenceClassification,pipeline,TextClassificationPipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold,cross_validate\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import make_scorer,accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tuning_script\n",
    "\n",
    "\n",
    "class Pipeline(TextClassificationPipeline):\n",
    "    def postprocess(self, model_outputs):\n",
    "        best_class = model_outputs[\"logits\"]\n",
    "        return best_class.squeeze().numpy()\n",
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "       \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation = True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding =  'max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "   \n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "        }\n",
    "    \n",
    "def read_data(filename,samp_size):\n",
    "     #reading Data\n",
    "    current_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    path = parent_dir +\"/model_data/\" + filename\n",
    "    df = pd.read_csv(path)\n",
    "  #Sampling data\n",
    "    if type(samp_size) is float:\n",
    "        if samp_size <=0:\n",
    "            samp_size = .1 \n",
    "        n = int(len(df) * samp_size)\n",
    "        if n > len(df):\n",
    "            print('sample size must be less than or equal to input size set sample to 4000')\n",
    "            n = 4000\n",
    "    else:\n",
    "        if samp_size > 1 and len(df) >= samp_size:\n",
    "            n = samp_size\n",
    "        else:\n",
    "            print('sample size must be less than or equal to input size set sample to 4000')\n",
    "            n = 4000\n",
    "        \n",
    "    df = df.sample(n = n)\n",
    "    df.reset_index(inplace=True, drop= True)\n",
    "    df.rename(columns={\"Review_text\":\"text\"},inplace=True)\n",
    "    df[\"Rating\"] = df[\"Rating\"].apply(encode_Test_Lables)\n",
    "    return df\n",
    "\n",
    "def encode_Test_Lables(label):\n",
    "    return int(label-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659356b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_set_1.csv\"\n",
    "samp_size = 5000\n",
    "def run_model(filename,samp_size,svm_C,meta_C,model_path):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "    data = read_data(filename,samp_size)\n",
    "    text = data[\"text\"].to_list()\n",
    "\n",
    "    #creating pipeline and predicting with roberta model\n",
    "    classifier = Pipeline(model = model,tokenizer = tokenizer)\n",
    "    tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "    pred = classifier(text,**tokenizer_kwargs)\n",
    "\n",
    "    roberta_pred = pd.DataFrame(pred)\n",
    "    roberta_pred = roberta_pred.to_numpy()\n",
    "\n",
    "    #Convert text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_df=.95,min_df=.0125)\n",
    "    svm_x = vectorizer.fit_transform(data[\"text\"])\n",
    "\n",
    "    # create model instance\n",
    "    classifier_svc = SVC(C=svm_C,class_weight='balanced',random_state = 50,probability=True)\n",
    "\n",
    "    # fit model\n",
    "    classifier_svc.fit(svm_x,data[\"Rating\"])\n",
    "\n",
    "    #predict\n",
    "    svm_results = classifier_svc.predict_proba(svm_x)\n",
    "\n",
    "    #Create meta data from roberta and svm predictions should be a num_models * (predictions * classes)\n",
    "    meta_data = np.concatenate((roberta_pred, svm_results), axis=1)\n",
    "\n",
    "    meta_svm = SVC(C=meta_C,class_weight='balanced',random_state = 50)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score,average=\"micro\",zero_division=0.0),\n",
    "        'recall': make_scorer(recall_score,average=\"micro\",zero_division=0.0),\n",
    "        'f1_score': make_scorer(f1_score,average=\"micro\",zero_division=0.0)\n",
    "    }      \n",
    "    cv_results = cross_validate(meta_svm, meta_data, data[\"Rating\"], cv=kf, scoring=scoring)\n",
    "    \n",
    "    print(\"Model Accuracy: {0:.2%} Precision: {1:.4f} Recall: {2:.4f} F1 Score: {3:.4f}\".format(np.mean(cv_results['test_accuracy']),np.mean(cv_results['test_f1_score']),np.mean(cv_results['test_precision']),np.mean(cv_results['test_recall'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(filename,samp_size,svm_C,meta_C,model_path):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "    data = read_data(filename,samp_size)\n",
    "    text = data[\"text\"].to_list()\n",
    "\n",
    "    #creating pipeline and predicting with roberta model\n",
    "    classifier = Pipeline(model = model,tokenizer = tokenizer)\n",
    "    tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "    pred = classifier(text,**tokenizer_kwargs)\n",
    "\n",
    "    roberta_pred = pd.DataFrame(pred)\n",
    "    roberta_pred = roberta_pred.to_numpy()\n",
    "\n",
    "    #Convert text data into numerical features using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_df=.95,min_df=.0125)\n",
    "    svm_x = vectorizer.fit_transform(data[\"text\"])\n",
    "\n",
    "    # create model instance\n",
    "    classifier_svc = SVC(C=svm_C,class_weight='balanced',random_state = 50,probability=True)\n",
    "\n",
    "    # fit model\n",
    "    classifier_svc.fit(svm_x,data[\"Rating\"])\n",
    "\n",
    "    #predict\n",
    "    svm_results = classifier_svc.predict_proba(svm_x)\n",
    "\n",
    "    #Create meta data from roberta and svm predictions should be a num_models * (predictions * classes)\n",
    "    meta_data = np.concatenate((roberta_pred, svm_results), axis=1)\n",
    "\n",
    "    meta_svm = SVC(C=.001,class_weight='balanced',random_state = 50)\n",
    "    \n",
    "    paramaters = {\n",
    "        'C' : meta_C\n",
    "    }\n",
    "    params = tuning_script.grid_search(meta_svm,paramaters,meta_data,data[\"Rating\"])\n",
    "    return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
